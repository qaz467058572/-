# -*- coding: utf-8 -*-
import aiosqlite
import asyncio
import numpy as np
import os
import sys
import gc
import json
import re
import requests
from typing import List, Dict, Any, Optional, Tuple
import threading

# ===================== å…¨å±€é…ç½® =====================
OPENAI_API_BASE = "http://198.18.0.1:1234/v1/responses"
OPENAI_API_KEY = ""
OPENAI_MODEL_NAME = "bubu1123"
OPENAI_TIMEOUT = 30
OPENAI_MAX_TOKENS = 64
OPENAI_TEMPERATURE = 0.0

# M3Eæ¨¡å‹é…ç½®ï¼ˆä¼˜å…ˆæœ¬åœ°ï¼Œæœ¬åœ°ä¸å­˜åœ¨åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹ï¼‰
M3E_MODEL_PATH = "./m3e-small" if os.path.exists("./m3e-small") else "all-MiniLM-L6-v2"

# æ•°æ®åº“ä¸å‘é‡é…ç½®
LOCAL_DB_IMPORT_PATH = "./boss_database.db"
TARGET_DB_PATH = "bosses.db"
VECTOR_STORAGE_PATH = "./boss_vectors.npy"
VECTOR_META_PATH = "./boss_vectors_meta.json"
BATCH_SIZE = 50
SHOW_PROGRESS = True
SIMILARITY_THRESHOLD = 0.8

# æœ¬åœ°æ•°æ®åº“å­—æ®µæ˜ å°„
FIELD_MAPPING = {
    "boss_name": "name",
    "location": "location",
    "level": "level",
    "attribute": "attribute",
    "physical_resistance": "physical_resistance",
    "magic_resistance": "magic_resistance",
    "physical_defense": "physical_defense",  # ç‰©ç†é˜²ç¦¦
    "magic_defense": "magic_defense",  # é­”æ³•é˜²ç¦¦
    "physical_inertia": "physical_inertia",  # ç‰©ç†æ…£æ€§
    "magic_inertia": "magic_inertia",  # é­”æ³•æ…£æ€§
    "general_inertia": "general_inertia",  # ä¸€èˆ¬æ…£æ€§
    "c_resistance": "c_resistance",  # CæŠµæŠ—
    "notes": "notes"
}

# ===================== å…¨å±€çŠ¶æ€ç®¡ç† =====================
# åˆå§‹åŒ–çŠ¶æ€æ ‡è®°ï¼ˆç¡®ä¿ä»…åˆå§‹åŒ–ä¸€æ¬¡ï¼‰
_SYSTEM_INITIALIZED = False
# åˆå§‹åŒ–é”ï¼ˆé˜²æ­¢å¹¶å‘è°ƒç”¨æ—¶é‡å¤åˆå§‹åŒ–ï¼‰
_INIT_LOCK = threading.Lock()


# ===================== æ¨¡å‹ç®¡ç†å™¨ =====================
class ModelManager:
    def __init__(self):
        self.m3e_model = None
        self.boss_vectors: Optional[np.ndarray] = np.array([])
        self.boss_vector_meta: Dict[str, int] = {}
        self.m3e_dim: int = 0
        self.vectors_loaded = False

    def init_m3e_model(self) -> bool:
        """
        åˆå§‹åŒ–M3Eå‘é‡æ¨¡å‹ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ç”Ÿæ•ˆï¼‰
        è¿”å›ï¼šæ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
        """
        if self.m3e_model is not None:
            print("âœ… M3Eæ¨¡å‹å·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True

        print("=== åˆå§‹åŒ–M3Eå‘é‡æ¨¡å‹ ===")
        try:
            from sentence_transformers import SentenceTransformer
            self.m3e_model = SentenceTransformer(M3E_MODEL_PATH)
            self.m3e_dim = len(self.m3e_model.encode("æµ‹è¯•æ–‡æœ¬"))
            print(f"âœ… M3EåŠ è½½å®Œæˆ | å‘é‡ç»´åº¦ï¼š{self.m3e_dim}")
            return True
        except Exception as e:
            print(f"âŒ M3EåŠ è½½å¤±è´¥ï¼š{e}")
            return False

    def _load_existing_vectors(self) -> bool:
        """
        åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“ï¼ˆé¿å…é‡å¤ç”Ÿæˆï¼‰
        è¿”å›ï¼šæ˜¯å¦åŠ è½½æˆåŠŸ
        """
        if self.vectors_loaded:
            return True

        try:
            if os.path.exists(VECTOR_STORAGE_PATH) and os.path.exists(VECTOR_META_PATH):
                self.boss_vectors = np.load(VECTOR_STORAGE_PATH)
                with open(VECTOR_META_PATH, "r", encoding="utf-8") as f:
                    self.boss_vector_meta = json.load(f)
                self.vectors_loaded = True
                print(f"âœ… åŠ è½½å·²æœ‰å‘é‡åº“ | å‘é‡æ•°é‡ï¼š{len(self.boss_vectors)}")
                return True
            else:
                print("âš ï¸ æœªæ‰¾åˆ°å·²å­˜åœ¨çš„å‘é‡åº“ï¼Œéœ€è¦é‡æ–°ç”Ÿæˆ")
                return False
        except Exception as e:
            print(f"âŒ åŠ è½½å‘é‡åº“å¤±è´¥ï¼š{e}")
            return False

    def _clean_old_vectors(self):
        """æ¸…ç†æ—§çš„å‘é‡åº“æ–‡ä»¶"""
        print("=== æ¸…ç†æ—§å‘é‡åº“æ•°æ® ===")
        try:
            if os.path.exists(VECTOR_STORAGE_PATH):
                os.remove(VECTOR_STORAGE_PATH)
                print(f"âœ… åˆ é™¤æ—§å‘é‡æ–‡ä»¶ï¼š{VECTOR_STORAGE_PATH}")
            if os.path.exists(VECTOR_META_PATH):
                os.remove(VECTOR_META_PATH)
                print(f"âœ… åˆ é™¤æ—§å‘é‡å…ƒæ•°æ®ï¼š{VECTOR_META_PATH}")
        except Exception as e:
            print(f"âš ï¸ åˆ é™¤æ—§å‘é‡åº“å¤±è´¥ï¼š{e}")

        self.boss_vectors = np.array([])
        self.boss_vector_meta = {}
        self.vectors_loaded = False

    def _save_vectors(self):
        """ä¿å­˜å‘é‡åº“"""
        if self.boss_vectors.size > 0:
            try:
                np.save(VECTOR_STORAGE_PATH, self.boss_vectors)
                with open(VECTOR_META_PATH, "w", encoding="utf-8") as f:
                    json.dump(self.boss_vector_meta, f, ensure_ascii=False)
                self.vectors_loaded = True
                print(f"âœ… å‘é‡åº“å·²ä¿å­˜åˆ°ï¼š{VECTOR_STORAGE_PATH}")
            except Exception as e:
                print(f"âŒ ä¿å­˜å‘é‡åº“å¤±è´¥ï¼š{e}")

    def cleanup(self):
        """é‡Šæ”¾èµ„æº"""
        self.m3e_model = None
        self.boss_vectors = None
        self.boss_vector_meta = {}
        self.vectors_loaded = False
        gc.collect()
        print("âœ… æ¨¡å‹èµ„æºé‡Šæ”¾å®Œæˆ")

    async def encode_text(self, text: str) -> np.ndarray:
        """
        å¼‚æ­¥ç¼–ç æ–‡æœ¬ä¸ºå‘é‡
        å‚æ•°ï¼štext - è¦ç¼–ç çš„æ–‡æœ¬
        è¿”å›ï¼šæ–‡æœ¬å¯¹åº”çš„å‘é‡
        """
        loop = asyncio.get_running_loop()
        vec = await loop.run_in_executor(
            None,
            self.m3e_model.encode,
            text
        )
        return vec


# ===================== å…¨å±€å®ä¾‹ =====================
model_manager = ModelManager()


# ===================== å·¥å…·å‡½æ•° =====================
def safe_float_convert(v: Any, default: float = 0.0) -> float:
    """å®‰å…¨è½¬æ¢ä¸ºæµ®ç‚¹æ•°"""
    try:
        return float(v) if v is not None else default
    except:
        return default


def safe_str_convert(v: Any, default: str = "æœªçŸ¥") -> str:
    """å®‰å…¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²"""
    return str(v).strip() if v and str(v).strip() else default


def cosine_similarity(query_vec: np.ndarray, corpus_vecs: np.ndarray) -> np.ndarray:
    """
    è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    å‚æ•°ï¼š
        query_vec - æŸ¥è¯¢å‘é‡
        corpus_vecs - è¯­æ–™åº“å‘é‡çŸ©é˜µ
    è¿”å›ï¼šç›¸ä¼¼åº¦æ•°ç»„
    """
    query_norm = np.linalg.norm(query_vec)
    corpus_norm = np.linalg.norm(corpus_vecs, axis=1)

    if query_norm == 0:
        return np.zeros(len(corpus_vecs))
    corpus_norm = np.where(corpus_norm == 0, 1e-8, corpus_norm)

    query_normalized = query_vec / query_norm
    corpus_normalized = corpus_vecs / corpus_norm[:, np.newaxis]

    return np.dot(corpus_normalized, query_normalized)


def _print_progress(current: int, total: int, prefix: str = ""):
    """æ‰“å°è¿›åº¦æ¡"""
    if not SHOW_PROGRESS:
        return
    percent = (current / total) * 100
    bar = "â–ˆ" * int(percent // 5) + "-" * (20 - int(percent // 5))
    print(f"\r{prefix} |{bar}| {current}/{total} ({percent:.1f}%)", end="")
    if current == total:
        print()


# ===================== æ ¸å¿ƒ1ï¼šå…³é”®è¯æå– =====================
def openai_api_get_keywords(raw_query: str) -> str:
    """æå–JSONç»“æ„åŒ–å…³é”®è¯"""
    prompt = f"""
    ä½ æ˜¯BOSSæ£€ç´¢å…³é”®è¯ç”Ÿæˆå™¨ï¼Œä¸¥æ ¼éµå®ˆä»¥ä¸‹è§„åˆ™è¾“å‡ºï¼š
    1. ä»…æå–ä¸BOSSã€åç§°ã€æ‰€åœ¨åœ°ã€å±æ€§ã€ç­‰çº§ã€‘ç›¸å…³çš„æ ¸å¿ƒè¯æ±‡
    2. å¿…é¡»è¾“å‡ºæ ‡å‡†JSONæ ¼å¼ï¼Œç»“æ„ä¸ºï¼š{{"keywords": ["è¯1", "è¯2", ...]}}
    3. JSONä¸­ä»…åŒ…å«keywordså­—æ®µï¼Œæ— å…¶ä»–å­—æ®µ
    4. ç¦æ­¢è¾“å‡ºJSONä»¥å¤–çš„ä»»ä½•å†…å®¹
    5. ç¤ºä¾‹ï¼š
       ç”¨æˆ·æŸ¥è¯¢ï¼š"æ‰¾æ‰¾ç©ºé”çš„é¾æ—BOSS"
       è¾“å‡ºï¼š{{"keywords": ["ç©ºé”", "é¾æ—"]}}

    ç”¨æˆ·æŸ¥è¯¢ï¼š{raw_query}
    """

    payload = {
        "input": prompt.strip(),
        "model": OPENAI_MODEL_NAME,
        "temperature": OPENAI_TEMPERATURE,
        "max_tokens": OPENAI_MAX_TOKENS,
        "stream": False
    }

    headers = {
        "Content-Type": "application/json"
    }

    try:
        response = requests.post(
            url=OPENAI_API_BASE,
            headers=headers,
            json=payload,
            timeout=OPENAI_TIMEOUT
        )
        response.raise_for_status()
        result = response.json()

        print(f"ğŸ“œ APIåŸå§‹è¿”å›ï¼š{json.dumps(result, ensure_ascii=False, indent=2)}")
        keywords = raw_query

        if "output" in result and isinstance(result["output"], list):
            for output_item in result["output"]:
                if output_item.get("type") == "message":
                    content_list = output_item.get("content", [])
                    for content_item in content_list:
                        if content_item.get("type") == "output_text":
                            text_content = content_item.get("text", "").strip()
                            if text_content:
                                try:
                                    clean_text = re.sub(r"^\s+|\s+$", "", text_content)
                                    json_data = json.loads(clean_text)
                                    keyword_list = json_data.get("keywords", [])
                                    if isinstance(keyword_list, list) and len(keyword_list) > 0:
                                        keywords = " ".join(keyword_list)
                                    break
                                except:
                                    keywords = text_content
                                    break

        keywords = re.sub(r"[\n\t\rï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š\"\'()ï¼ˆï¼‰ã€ã€‘]", " ", keywords)
        keywords = re.sub(r"\s+", " ", keywords).strip()
        print(f"âœ… æœ€ç»ˆæå–å…³é”®è¯ï¼š{keywords}")

        return keywords
    except Exception as e:
        print(f"âŒ APIè°ƒç”¨å¤±è´¥ï¼š{str(e)}")
        return raw_query


# ===================== æ ¸å¿ƒ2ï¼šæ•°æ®åº“åˆå§‹åŒ– =====================
async def _init_database_if_needed() -> bool:
    """
    åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä»…åœ¨æ•°æ®åº“ä¸å­˜åœ¨æˆ–ä¸ºç©ºæ—¶æ‰§è¡Œï¼‰
    è¿”å›ï¼šæ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
    """
    # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨ä¸”æœ‰æ•°æ®
    try:
        if os.path.exists(TARGET_DB_PATH):
            async with aiosqlite.connect(TARGET_DB_PATH) as db:
                cursor = await db.execute("SELECT COUNT(*) FROM bosses")
                count = (await cursor.fetchone())[0]
                if count > 0:
                    print(f"âœ… æ•°æ®åº“å·²å­˜åœ¨ä¸”æœ‰{count}æ¡æ•°æ®ï¼Œè·³è¿‡æ•°æ®åº“åˆå§‹åŒ–")
                    return True
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥æ•°æ®åº“å¤±è´¥ï¼š{e}")

    print("=== åˆå§‹åŒ–æ•°æ®åº“ ===")
    try:
        # åˆ é™¤æ—§æ•°æ®åº“æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ä½†æ— æ•°æ®ï¼‰
        if os.path.exists(TARGET_DB_PATH):
            os.remove(TARGET_DB_PATH)
            print(f"âœ… åˆ é™¤æ— æ•ˆæ•°æ®åº“æ–‡ä»¶ï¼š{TARGET_DB_PATH}")

        # é‡å»ºè¡¨ç»“æ„
        async with aiosqlite.connect(TARGET_DB_PATH) as db:
            await db.execute("DROP TABLE IF EXISTS bosses")
            await db.execute("""
                CREATE TABLE IF NOT EXISTS bosses (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    boss_name TEXT NOT NULL,
                    location TEXT,
                    level REAL,
                    attribute TEXT,
                    physical_resistance REAL,
                    magic_resistance REAL,
                    physical_defense REAL,
                    magic_defense REAL,
                    physical_inertia REAL,
                    magic_inertia REAL,
                    general_inertia REAL,
                    c_resistance REAL,
                    notes TEXT
                )
            """)
            await db.commit()

        # å¯¼å…¥æ•°æ®
        imported_count = await _import_local_database()
        if imported_count == 0:
            print("âš ï¸ æ— å¯ç”¨çš„BOSSæ•°æ®ï¼Œæ•°æ®åº“åˆå§‹åŒ–å®Œæˆä½†æ— æ•°æ®")
        else:
            print("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
        return False


async def _import_local_database() -> int:
    """
    ä»æœ¬åœ°æ•°æ®åº“å¯¼å…¥BOSSæ•°æ®
    è¿”å›ï¼šå¯¼å…¥çš„æ•°æ®æ¡æ•°
    """
    if not os.path.exists(LOCAL_DB_IMPORT_PATH):
        print(f"âš ï¸ æœ¬åœ°æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼š{LOCAL_DB_IMPORT_PATH}")
        return 0

    try:
        async with aiosqlite.connect(LOCAL_DB_IMPORT_PATH) as local_db:
            cursor = await local_db.execute("PRAGMA table_info(bosses)")
            local_fields = [row[1] for row in await cursor.fetchall()]

            valid_fields = []
            target_fields = []
            for target_field, local_field in FIELD_MAPPING.items():
                target_fields.append(target_field)
                if local_field in local_fields:
                    valid_fields.append(local_field)
                else:
                    print(f"âš ï¸ æœ¬åœ°æ•°æ®åº“ç¼ºå°‘å­—æ®µï¼š{local_field}ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼")
                    valid_fields.append("''")

            select_sql = f"SELECT {', '.join(valid_fields)} FROM bosses"
            cursor = await local_db.execute(select_sql)
            local_data = await cursor.fetchall()
            if len(local_data) == 0:
                print(f"âš ï¸ æœ¬åœ°æ•°æ®åº“ä¸­æ— BOSSæ•°æ®")
                return 0

            async with aiosqlite.connect(TARGET_DB_PATH) as target_db:
                await target_db.execute("DELETE FROM bosses")
                insert_sql = f"""
                    INSERT INTO bosses ({', '.join(target_fields)})
                    VALUES ({', '.join(['?' for _ in target_fields])})
                """
                await target_db.executemany(insert_sql, local_data)
                await target_db.commit()

                print(f"âœ… æˆåŠŸä»æœ¬åœ°æ•°æ®åº“å¯¼å…¥ {len(local_data)} æ¡BOSSæ•°æ®")
                return len(local_data)

    except Exception as e:
        print(f"âŒ å¯¼å…¥æœ¬åœ°æ•°æ®åº“å¤±è´¥ï¼š{e}")
        return 0


# ===================== æ ¸å¿ƒ3ï¼šå‘é‡åŒæ­¥ =====================
async def _sync_boss_vectors_if_needed() -> Dict[str, Any]:
    """
    åŒæ­¥BOSSå‘é‡åº“ï¼ˆä»…åœ¨å‘é‡åº“ä¸å­˜åœ¨æ—¶æ‰§è¡Œï¼‰
    è¿”å›ï¼šåŒæ­¥ç»“æœå­—å…¸
    """
    # å°è¯•åŠ è½½å·²æœ‰å‘é‡
    if model_manager._load_existing_vectors():
        return {
            "status": "success",
            "processed": len(model_manager.boss_vectors),
            "total": len(model_manager.boss_vectors),
            "msg": "ä½¿ç”¨å·²æœ‰å‘é‡åº“ï¼Œæ— éœ€é‡æ–°ç”Ÿæˆ"
        }

    # å…ˆåˆå§‹åŒ–æ•°æ®åº“
    db_init_ok = await _init_database_if_needed()
    if not db_init_ok:
        return {"status": "failed", "processed": 0, "total": 0, "msg": "æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥"}

    # è·å–BOSSæ•°æ®
    async with aiosqlite.connect(TARGET_DB_PATH) as db:
        cursor = await db.execute("SELECT id, boss_name, location, level, attribute FROM bosses")
        rows = await cursor.fetchall()
        total = len(rows)
        if total == 0:
            return {"status": "failed", "processed": 0, "total": 0, "msg": "æ— BOSSæ•°æ®"}

    # æ¸…ç†æ—§å‘é‡ï¼ˆç¡®ä¿ç”Ÿæˆæ–°å‘é‡ï¼‰
    model_manager._clean_old_vectors()

    print("=== å¼€å§‹ç”ŸæˆBOSSå‘é‡åº“ ===")
    batch_vectors = []
    processed = 0

    for row in rows:
        try:
            boss_id = row[0]
            boss_name = row[1]
            location = row[2]
            attribute = row[4]

            # ç”Ÿæˆå‘é‡æ–‡æœ¬
            boss_text = f"BOSSåç§°ï¼š{boss_name} æ‰€åœ¨åœ°ï¼š{location} å±æ€§ï¼š{attribute}"
            vec = await model_manager.encode_text(boss_text)
            batch_vectors.append(vec)
            model_manager.boss_vector_meta[str(boss_id)] = processed
            processed += 1
            _print_progress(processed, total, "ğŸ“¤ å‘é‡åŒæ­¥è¿›åº¦")

            # æ‰¹é‡ä¿å­˜
            if len(batch_vectors) >= BATCH_SIZE or processed == total:
                vec_arr = np.array(batch_vectors)
                if model_manager.boss_vectors.size == 0:
                    model_manager.boss_vectors = vec_arr
                else:
                    model_manager.boss_vectors = np.vstack([model_manager.boss_vectors, vec_arr])
                batch_vectors = []
        except Exception as e:
            print(f"\nâŒ å¤„ç†BOSS {row[1]} å¤±è´¥ï¼š{e}")
            continue

    # ä¿å­˜æœ€ç»ˆå‘é‡
    model_manager._save_vectors()

    return {
        "status": "success",
        "processed": processed,
        "total": total,
        "msg": f"æˆåŠŸç”Ÿæˆ {processed}/{total} æ¡BOSSå‘é‡"
    }


# ===================== æ ¸å¿ƒ4ï¼šBOSSæ£€ç´¢ =====================
async def _do_search(keywords: str, limit: int = 5, threshold: float = SIMILARITY_THRESHOLD) -> List[Dict]:
    """
    å®é™…æ‰§è¡Œæ£€ç´¢é€»è¾‘ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰
    """
    # æ‹†åˆ†å…³é”®è¯
    keyword_list = list(set([k.strip() for k in keywords.split() if k.strip()]))
    if not keyword_list:
        return []

    # ç¬¬ä¸€æ­¥ï¼šç²¾å‡†æ–‡æœ¬åŒ¹é…
    async with aiosqlite.connect(TARGET_DB_PATH) as db:
        where_conditions = []
        params = []
        for keyword in keyword_list:
            where_conditions.append("(boss_name LIKE ? OR location LIKE ? OR attribute LIKE ? OR notes LIKE ?)")
            params.extend([f"%{keyword}%", f"%{keyword}%", f"%{keyword}%", f"%{keyword}%"])

        where_sql = " AND ".join(where_conditions)
        query_sql = f"""
            SELECT id, boss_name, location, level, attribute, physical_resistance, magic_resistance,
            physical_defense, magic_defense, physical_inertia, magic_inertia, general_inertia, c_resistance, notes 
            FROM bosses 
            WHERE {where_sql}
        """

        cursor = await db.execute(query_sql, params)
        text_match_results = await cursor.fetchall()

    # å¤„ç†ç²¾å‡†åŒ¹é…ç»“æœ
    if text_match_results:
        weighted_results = []
        for row in text_match_results:
            boss_name = row[1]
            location = row[2]
            attribute = row[4] if row[4] else ""
            notes = row[13] if row[13] else ""
            total_text = f"{boss_name}{location}{attribute}{notes}"

            # è®¡ç®—åŒ¹é…æƒé‡
            match_count = 0
            for keyword in keyword_list:
                if keyword in total_text:
                    match_count += 1

            weighted_results.append({
                "row": row,
                "match_count": match_count
            })

        # æ’åºå¹¶æ„å»ºç»“æœ
        weighted_results.sort(key=lambda x: x["match_count"], reverse=True)
        results = []
        for idx, item in enumerate(weighted_results[:limit]):
            row = item["row"]
            results.append({
                "æ’å": idx + 1,
                "åç¨±": row[1],
                "æ‰€åœ¨åœ°": row[2],
                "ç­‰ç´š": row[3],
                "å±¬æ€§": row[4],
                "ç‰©ç†æŠ—æ€§": row[5],
                "é­”æ³•æŠ—æ€§": row[6],
                "ç‰©ç†é˜²ç¦¦": row[7],
                "é­”æ³•é˜²ç¦¦": row[8],
                "ç‰©ç†æ…£æ€§": row[9],
                "é­”æ³•æ…£æ€§": row[10],
                "ä¸€èˆ¬æ…£æ€§": row[11],
                "CæŠµæŠ—": row[12],
                "ç›¸ä¼¼åº¦": round(1.0 - (0.1 * (len(keyword_list) - item["match_count"])), 3),
                "å‚™è¨»": row[13]
            })
        return results

    # ç¬¬äºŒæ­¥ï¼šå‘é‡æ£€ç´¢å…œåº•
    if model_manager.boss_vectors.size == 0:
        print("âŒ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œå‘é‡æ£€ç´¢")
        return []

    # ç”ŸæˆæŸ¥è¯¢å‘é‡
    query_text = f"BOSSåç§°ï¼š{keywords} æ‰€åœ¨åœ°ï¼š{keywords} å±æ€§ï¼š{keywords}"
    query_vec = await model_manager.encode_text(query_text)

    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = cosine_similarity(query_vec, model_manager.boss_vectors)

    # ç­›é€‰ç»“æœ
    valid_mask = similarities >= threshold
    valid_idx = np.where(valid_mask)[0]
    if len(valid_idx) == 0:
        return []

    sorted_idx = valid_idx[np.argsort(-similarities[valid_idx])][:limit]

    # æŸ¥è¯¢æ•°æ®åº“è·å–è¯¦ç»†ä¿¡æ¯
    async with aiosqlite.connect(TARGET_DB_PATH) as db:
        boss_ids = [list(model_manager.boss_vector_meta.keys())[int(i)] for i in sorted_idx]
        placeholders = ",".join(["?"] * len(boss_ids))
        cursor = await db.execute(f"""
            SELECT id, boss_name, location, level, attribute, physical_resistance, magic_resistance,
            physical_defense, magic_defense, physical_inertia, magic_inertia, general_inertia, c_resistance, notes 
            FROM bosses WHERE id IN ({placeholders})
        """, boss_ids)
        boss_rows = await cursor.fetchall()

    # æ„å»ºç»“æœ
    boss_map = {str(row[0]): row for row in boss_rows}
    results = []
    for idx, vec_idx in enumerate(sorted_idx):
        boss_id = list(model_manager.boss_vector_meta.keys())[int(vec_idx)]
        boss = boss_map.get(boss_id)
        if boss:
            results.append({
                "æ’å": idx + 1,
                "åç¨±": boss[1],
                "æ‰€åœ¨åœ°": boss[2],
                "ç­‰ç´š": boss[3],
                "å±¬æ€§": boss[4],
                "ç‰©ç†æŠ—æ€§": boss[5],
                "é­”æ³•æŠ—æ€§": boss[6],
                "ç‰©ç†é˜²ç¦¦": boss[7],
                "é­”æ³•é˜²ç¦¦": boss[8],
                "ç‰©ç†æ…£æ€§": boss[9],
                "é­”æ³•æ…£æ€§": boss[10],
                "ä¸€èˆ¬æ…£æ€§": boss[11],
                "CæŠµæŠ—": boss[12],
                "ç›¸ä¼¼åº¦": round(float(similarities[vec_idx]), 3),
                "å‚™è¨»": boss[13]
            })
    return results


# ===================== å¯¹å¤–å…¬å¼€çš„æ ¸å¿ƒæ¥å£ =====================
async def init_system(force_reinit: bool = False) -> bool:
    """
    åˆå§‹åŒ–æ•´ä¸ªBOSSæ£€ç´¢ç³»ç»Ÿï¼ˆä»…é¦–æ¬¡è°ƒç”¨ç”Ÿæ•ˆï¼Œå¯å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ï¼‰
    å‚æ•°ï¼š
        force_reinit - æ˜¯å¦å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ï¼ˆé»˜è®¤Falseï¼‰
    è¿”å›ï¼šæ˜¯å¦åˆå§‹åŒ–æˆåŠŸ
    """
    global _SYSTEM_INITIALIZED

    # åŠ é”é˜²æ­¢å¹¶å‘åˆå§‹åŒ–
    with _INIT_LOCK:
        # å¦‚æœå·²åˆå§‹åŒ–ä¸”ä¸å¼ºåˆ¶é‡æ–°åˆå§‹åŒ–ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if _SYSTEM_INITIALIZED and not force_reinit:
            print("âœ… ç³»ç»Ÿå·²åˆå§‹åŒ–ï¼Œè·³è¿‡é‡å¤åˆå§‹åŒ–")
            return True

        print("=== åˆå§‹åŒ–BOSSæ£€ç´¢ç³»ç»Ÿ ===")

        # 1. åˆå§‹åŒ–æ¨¡å‹
        model_ok = model_manager.init_m3e_model()
        if not model_ok:
            return False

        # 2. åŒæ­¥å‘é‡åº“ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        sync_result = await _sync_boss_vectors_if_needed()
        if sync_result["status"] != "success":
            print(f"âŒ å‘é‡åº“åŒæ­¥å¤±è´¥ï¼š{sync_result['msg']}")
            return False

        # æ ‡è®°ä¸ºå·²åˆå§‹åŒ–
        _SYSTEM_INITIALIZED = True
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        return True


async def search_boss(raw_query: str, limit: int = 5) -> List[Dict]:
    """
    å¯¹å¤–å…¬å¼€çš„BOSSæ£€ç´¢æ¥å£ï¼ˆè‡ªåŠ¨æ£€æŸ¥å¹¶åˆå§‹åŒ–ç³»ç»Ÿï¼‰
    å‚æ•°ï¼š
        raw_query - ç”¨æˆ·åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
        limit - è¿”å›ç»“æœæ•°é‡
    è¿”å›ï¼šæ£€ç´¢ç»“æœåˆ—è¡¨
    """
    # è‡ªåŠ¨åˆå§‹åŒ–ï¼ˆä»…é¦–æ¬¡è°ƒç”¨ï¼‰
    await init_system()

    # æå–å…³é”®è¯
    keywords = openai_api_get_keywords(raw_query)
    if not keywords:
        keywords = raw_query

    # æ‰§è¡Œæ£€ç´¢
    results = await _do_search(keywords, limit, SIMILARITY_THRESHOLD)
    return results


async def cleanup_system():
    """
    æ¸…ç†ç³»ç»Ÿèµ„æºï¼ˆé‡ç½®åˆå§‹åŒ–çŠ¶æ€ï¼‰
    """
    global _SYSTEM_INITIALIZED

    with _INIT_LOCK:
        model_manager.cleanup()
        _SYSTEM_INITIALIZED = False
        print("âœ… ç³»ç»Ÿèµ„æºå·²æ¸…ç†ï¼Œåˆå§‹åŒ–çŠ¶æ€å·²é‡ç½®")


def is_system_initialized() -> bool:
    """
    æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–
    è¿”å›ï¼šåˆå§‹åŒ–çŠ¶æ€
    """
    return _SYSTEM_INITIALIZED

async def search_by_keywords(keywords: str, limit: int = 5, threshold: float = SIMILARITY_THRESHOLD) -> List[Dict]:
    """
    å…¼å®¹æ—§ç‰ˆæœ¬çš„æ£€ç´¢æ¥å£ï¼ˆå¯¹å¤–æš´éœ²ï¼‰
    """
    # ç¡®ä¿ç³»ç»Ÿå·²åˆå§‹åŒ–
    await init_system()
    # è°ƒç”¨å†…éƒ¨æ£€ç´¢é€»è¾‘
    return await _do_search(keywords, limit, threshold)